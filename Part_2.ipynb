{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF3sLETqiAsqUeVqEweU3q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleniHaylemeskel/Week-3-AI-Tools-Assignment/blob/main/Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJRYjSVYOWZ0"
      },
      "outputs": [],
      "source": [
        "# PART 2 - TASK 1: Classical Machine Learning with Scikit-learn\n",
        "# Dataset: Iris Species\n",
        "# Goal: Preprocess â†’ Train Decision Tree â†’ Evaluate\n",
        "\n",
        "\n",
        "# --- Import Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "\n",
        "# --- Step 1: Load Dataset ---\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# Remove unnamed or unnecessary columns\n",
        "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "print(\"Dataset Loaded Successfully!\")\n",
        "print(df.head())\n",
        "\n",
        "# --- Step 2: Check & Handle Missing Values ---\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
        "\n",
        "# Fill missing numeric values with median (if any)\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "for col in num_cols:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "        print(f\"Filled missing values in column: {col}\")\n",
        "\n",
        "# --- Step 3: Encode Label Column ---\n",
        "label_col = \"Species\"\n",
        "le = LabelEncoder()\n",
        "df[label_col] = le.fit_transform(df[label_col])\n",
        "\n",
        "print(\"\\nLabel Encoding Completed!\")\n",
        "print(\"Classes:\", le.classes_)\n",
        "\n",
        "# --- Step 4: Prepare Features and Labels ---\n",
        "# Remove ID column if present\n",
        "feature_cols = [col for col in df.columns if col not in [\"Species\", \"Id\"]]\n",
        "\n",
        "X = df[feature_cols]\n",
        "y = df[label_col]\n",
        "\n",
        "# --- Step 5: Split Train & Test ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\nData Split Completed!\")\n",
        "print(\"Training Shape:\", X_train.shape)\n",
        "print(\"Test Shape:\", X_test.shape)\n",
        "\n",
        "# --- Step 6: Train the Decision Tree Classifier ---\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nModel Training Completed!\")\n",
        "\n",
        "# --- Step 7: Predictions ---\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# --- Step 8: Evaluation ---\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
        "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "print(\"\\n=== MODEL PERFORMANCE ===\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Macro): {precision:.4f}\")\n",
        "print(f\"Recall (Macro): {recall:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nTask 1 Completed Successfully! ğŸ‰\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2 â€“ TASK 2: Deep Learning with TensorFlow (MNIST CNN)\n",
        "# Dataset:Â MNIST Handwritten Digits\n",
        "# Goal: Build a Model â†’ Test Accuracy â†’ Visualize the Model\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 1. Load and Preprocess Data\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values (0â€“255 â†’ 0â€“1)\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Reshape for CNN: (batch, width, height, channels)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "\n",
        "# 2. Build CNN Model\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # 10 digits\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# 3. Train CNN Model\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Evaluate on Test Set\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"\\nTest Accuracy:\", test_acc)\n",
        "\n",
        "\n",
        "# 5. Visualize Predictions (5 Sample Images)\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "def plot_sample(i):\n",
        "    plt.imshow(x_test[i].reshape(28,28), cmap=\"gray\")\n",
        "    plt.title(f\"Prediction: {np.argmax(predictions[i])}\\nActual: {y_test[i]}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Show 5 random samples\n",
        "for i in np.random.randint(0, len(x_test), 5):\n",
        "    plot_sample(i)\n"
      ],
      "metadata": {
        "id": "I9EhMaBwX9uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: NLP with spaCy\n",
        "# Text Data: User reviews fromÂ Amazon Product Reviews\n",
        "# Goal: Extract product names/brands using NER + sentiment rules\n",
        "\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# --- Sample Amazon-style reviews ---\n",
        "reviews = [\n",
        "    \"I love my new Samsung Galaxy phone. The camera quality is amazing!\",\n",
        "    \"The Apple AirPods are overpriced and the battery dies quickly.\",\n",
        "    \"This Sony speaker has incredible bass. Totally worth the price!\",\n",
        "    \"I bought a Dell laptop and it keeps overheating. Not happy with it.\",\n",
        "    \"The Nike running shoes are very comfortable and durable.\"\n",
        "]\n",
        "\n",
        "# --- Rule-based sentiment analyzer ---\n",
        "positive_keywords = [\"love\", \"great\", \"amazing\", \"incredible\", \"worth\", \"comfortable\", \"durable\"]\n",
        "negative_keywords = [\"bad\", \"overpriced\", \"dies\", \"poor\", \"overheating\", \"not happy\"]\n",
        "\n",
        "def rule_based_sentiment(text):\n",
        "    text_lower = text.lower()\n",
        "    score = 0\n",
        "\n",
        "    for word in positive_keywords:\n",
        "        if word in text_lower:\n",
        "            score += 1\n",
        "\n",
        "    for word in negative_keywords:\n",
        "        if word in text_lower:\n",
        "            score -= 1\n",
        "\n",
        "    if score > 0:\n",
        "        return \"Positive\"\n",
        "    elif score < 0:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# --- Process each review ---\n",
        "for review in reviews:\n",
        "    doc = nlp(review)\n",
        "\n",
        "    print(\"\\nReview:\", review)\n",
        "\n",
        "    # Extract product-related entities\n",
        "    print(\"Named Entities (Product/Brand):\")\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in [\"ORG\", \"PRODUCT\"]:\n",
        "            print(f\"  - {ent.text} ({ent.label_})\")\n",
        "\n",
        "    # Sentiment result\n",
        "    sentiment = rule_based_sentiment(review)\n",
        "    print(\"Sentiment:\", sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L88bxAcRfUWV",
        "outputId": "084fa104-57b7-4ea1-dd40-654d7766115f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Review: I love my new Samsung Galaxy phone. The camera quality is amazing!\n",
            "Named Entities (Product/Brand):\n",
            "  - Samsung Galaxy (ORG)\n",
            "Sentiment: Positive\n",
            "\n",
            "Review: The Apple AirPods are overpriced and the battery dies quickly.\n",
            "Named Entities (Product/Brand):\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: This Sony speaker has incredible bass. Totally worth the price!\n",
            "Named Entities (Product/Brand):\n",
            "  - Sony (ORG)\n",
            "Sentiment: Positive\n",
            "\n",
            "Review: I bought a Dell laptop and it keeps overheating. Not happy with it.\n",
            "Named Entities (Product/Brand):\n",
            "  - Dell (ORG)\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: The Nike running shoes are very comfortable and durable.\n",
            "Named Entities (Product/Brand):\n",
            "  - Nike (ORG)\n",
            "Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(28,28)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "j-f9Jj0qf71x",
        "outputId": "7aa5839c-2863-46db-f4ae-8e4b8c3f1185"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 28, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(32, 28, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(32,), output.shape=(32, 28, 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-549932674.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/nn.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    660\u001b[0m         )\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0;34m\"Arguments `target` and `output` must have the same rank \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;34m\"(ndim). Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(32,), output.shape=(32, 28, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Correct input shape: flatten 28x28 into 784\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')   # 10 digit classes\n",
        "])\n",
        "\n",
        "# Correct loss for integer labels\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the repaired model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4OPza8pl1pA",
        "outputId": "d62a8cf7-6a0c-4057-9821-e1f0ab621c7e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8506 - loss: 0.5139\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9632 - loss: 0.1247\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9761 - loss: 0.0780\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9817 - loss: 0.0590\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9856 - loss: 0.0453\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7876dc202cf0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}